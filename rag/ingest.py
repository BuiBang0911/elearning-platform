import os
import re
import unicodedata
import hashlib
import datetime
import io
import shutil
import fitz  # PyMuPDF
import psycopg2
import psycopg2.extras
from dotenv import load_dotenv
from PIL import Image, ImageEnhance, ImageOps
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_postgres import PGVector
from rapidocr_onnxruntime import RapidOCR
from langchain_core.documents import Document

# --- C·∫§U H√åNH DEBUG ---
DEBUG_MODE = True  # Chuy·ªÉn sang False ƒë·ªÉ t·∫Øt debug
DEBUG_FOLDER = "./debug_output"

if DEBUG_MODE:
    if os.path.exists(DEBUG_FOLDER):
        try: shutil.rmtree(DEBUG_FOLDER)
        except: pass
    os.makedirs(DEBUG_FOLDER, exist_ok=True)
    print(f"üêû DEBUG MODE: ON. ·∫¢nh s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o '{DEBUG_FOLDER}'")

# 1. Load m√¥i tr∆∞·ªùng
load_dotenv()
DB_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = "my_docs" 

# --- 1. KH·ªûI T·∫†O OCR ---
ocr = RapidOCR()

# --- H√ÄM L·ªåC HEADER R√ÅC C·ª¶A PDF ---
def remove_pdf_artifacts(text):
    """
    H√†m n√†y lo·∫°i b·ªè c√°c d√≤ng header/footer l·∫∑p l·∫°i g√¢y nhi·ªÖu.
    B·∫°n c√≥ th·ªÉ th√™m c√°c t·ª´ kh√≥a header c·ª• th·ªÉ c·ªßa t√†i li·ªáu v√†o ƒë√¢y.
    """
    lines = text.split('\n')
    cleaned_lines = []
    
    for line in lines:
        line_clean = line.strip()
        # 1. B·ªè d√≤ng tr·ªëng
        if not line_clean: continue
        
        # 2. B·ªè c√°c Header/Footer c·ª• th·ªÉ (D·ª±a tr√™n t√†i li·ªáu b·∫°n g·ª≠i)
        # V√≠ d·ª•: "AI VIETNAM (AIO2024)", "aivietnam.edu.vn"
        if "AI VIETNAM" in line_clean: continue
        if "aivietnam.edu.vn" in line_clean: continue
        if "AI COURSE 2024" in line_clean: continue
        
        # 3. B·ªè s·ªë trang ƒë∆°n l·∫ª (v√≠ d·ª• d√≤ng ch·ªâ c√≥ s·ªë "1", "2")
        if line_clean.isdigit() and len(line_clean) < 4: continue
        
        cleaned_lines.append(line) # Gi·ªØ nguy√™n format d√≤ng ƒë·ªÉ n·ªëi sau
        
    return "\n".join(cleaned_lines)

# --- 2. C√ÅC H√ÄM X·ª¨ L√ù TEXT ---

def clean_and_merge_lines(text):
    """L√†m s·∫°ch v√† n·ªëi d√≤ng vƒÉn b·∫£n b·ªã ng·∫Øt sai."""
    if not text: return ""
    text = remove_pdf_artifacts(text)
    text = unicodedata.normalize('NFC', text)
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    text = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def adaptive_text_sorting(ocr_result):
    """
    T·ª± ƒë·ªông ph√°t hi·ªán b·ªë c·ª•c:
    - N·∫øu c√≥ kho·∫£ng tr·∫Øng d·ªçc xuy√™n su·ªët -> ƒê·ªçc theo C·ªòT (S∆° ƒë·ªì).
    - N·∫øu text tr·∫£i d√†i li√™n t·ª•c -> ƒê·ªçc theo D√íNG (VƒÉn b·∫£n th∆∞·ªùng).
    """
    if not ocr_result: return ""
    
    # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
    boxes = []
    min_x, max_x = 10000, 0
    
    for item in ocr_result:
        box, text = item[0], item[1]
        xs = [pt[0] for pt in box]
        ys = [pt[1] for pt in box]
        x1, x2 = min(xs), max(xs)
        y1, y2 = min(ys), max(ys)
        
        boxes.append({
            "text": text,
            "x1": x1, "x2": x2,
            "y1": y1, "y2": y2,
            "cx": (x1+x2)/2, "cy": (y1+y2)/2
        })
        min_x = min(min_x, x1)
        max_x = max(max_x, x2)

    # 2. KI·ªÇM TRA "KHE H·ªû D·ªåC" (VERTICAL GAPS)
    # T·∫°o m·ªôt m·∫£ng ƒë·∫°i di·ªán cho tr·ª•c X
    width = int(max_x - min_x) + 1
    if width <= 0: return ""
    
    x_projection = [0] * width # 0 l√† tr·ªëng, 1 l√† c√≥ ch·ªØ
    
    for b in boxes:
        # Chi·∫øu h·ªôp ch·ªØ xu·ªëng tr·ª•c X
        start = int(b['x1'] - min_x)
        end = int(b['x2'] - min_x)
        for k in range(max(0, start), min(width, end)):
            x_projection[k] = 1

    # T√¨m c√°c kho·∫£ng tr·ªëng l·ªõn tr√™n tr·ª•c X (Gap > 20px)
    GAP_THRESHOLD = 30
    has_vertical_split = False
    current_gap = 0
    
    # Ch·ªâ x√©t v√πng gi·ªØa (b·ªè qua l·ªÅ tr√°i/ph·∫£i)
    margin = int(width * 0.1) 
    for val in x_projection[margin : width - margin]:
        if val == 0:
            current_gap += 1
        else:
            if current_gap > GAP_THRESHOLD:
                has_vertical_split = True
                break
            current_gap = 0
            
    # Check l·∫ßn cu·ªëi n·∫øu gap n·∫±m ·ªü cu·ªëi
    if current_gap > GAP_THRESHOLD: has_vertical_split = True

    # 3. QUY·∫æT ƒê·ªäNH CHI·∫æN THU·∫¨T
    if has_vertical_split:
        # === CH·∫æ ƒê·ªò C·ªòT (COLUMN MODE) ===
        # D√†nh cho S∆° ƒë·ªì RAG, B·∫£ng bi·ªÉu
        print("         ‚ö° Ph√°t hi·ªán b·ªë c·ª•c C·ªòT -> Gom nh√≥m d·ªçc.")
        
        # Gom nh√≥m d·ª±a tr√™n t√¢m X (Center X)
        boxes.sort(key=lambda k: k['cx'])
        columns = []
        current_col = [boxes[0]]
        
        COL_MARGIN = 50 # C√°c ch·ªØ l·ªách nhau < 50px th√¨ c√πng c·ªôt
        
        for i in range(1, len(boxes)):
            prev, curr = current_col[-1], boxes[i]
            if abs(curr['cx'] - prev['cx']) < COL_MARGIN:
                current_col.append(curr)
            else:
                columns.append(current_col)
                current_col = [curr]
        if current_col: columns.append(current_col)

        final_text = []
        for col in columns:
            # Trong m·ªói c·ªôt, s·∫Øp x·∫øp t·ª´ tr√™n xu·ªëng d∆∞·ªõi
            col.sort(key=lambda k: k['cy'])
            col_text = " ".join([b['text'] for b in col])
            final_text.append(col_text)
            
        return "\n".join(final_text)

    else:
        # === CH·∫æ ƒê·ªò D√íNG (ROW MODE) - M·∫∂C ƒê·ªäNH ===
        # D√†nh cho vƒÉn b·∫£n th∆∞·ªùng, paragraph
        print("         üìù Ph√°t hi·ªán b·ªë c·ª•c VƒÇN B·∫¢N -> ƒê·ªçc theo d√≤ng.")
        
        # S·∫Øp x·∫øp theo Y tr∆∞·ªõc (ƒë·ªÉ gom d√≤ng), sau ƒë√≥ theo X
        # RapidOCR m·∫∑c ƒë·ªãnh tr·∫£ v·ªÅ kh√° chu·∫©n, ta ch·ªâ c·∫ßn sort nh·∫π l·∫°i
        
        # Logic ƒë∆°n gi·∫£n: Sort theo Y top-down. 
        # N·∫øu Y g·∫ßn nhau (<10px) th√¨ sort theo X left-right.
        boxes.sort(key=lambda k: (int(k['cy'] / 15), k['cx'])) 
        
        return " ".join([b['text'] for b in boxes])
    
def process_image_for_ocr(img_bytes, debug_name=None):
    """
    H√†m x·ª≠ l√Ω ·∫£nh to√†n di·ªán:
    1. Ti·ªÅn x·ª≠ l√Ω (N·ªÅn tr·∫Øng, T∆∞∆°ng ph·∫£n).
    2. Ch·∫°y OCR.
    3. S·∫Øp x·∫øp th√¥ng minh (Adaptive Sorting).
    """
    try:
        image = Image.open(io.BytesIO(img_bytes))
        print(f"      üñºÔ∏è  X·ª≠ l√Ω ·∫£nh k√≠ch th∆∞·ªõc: {image.size}, mode: {image.mode}")
        # 1. L√ìT N·ªÄN TR·∫ÆNG (Fix l·ªói trong su·ªët)
        if image.mode in ('RGBA', 'LA') or (image.mode == 'P' and 'transparency' in image.info):
            bg = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode != 'RGBA': image = image.convert('RGBA')
            bg.paste(image, mask=image.split()[3])
            image = bg
        else:
            image = image.convert('RGB')

        # 2. TƒÇNG T∆Ø∆†NG PH·∫¢N
        image = ImageOps.grayscale(image)
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(2.0) 

        print("         üîç ·∫¢nh ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω cho OCR.")
        # [Debug] L∆∞u ·∫£nh x·ª≠ l√Ω
        if DEBUG_MODE and debug_name:
            try:
                image.save(os.path.join(DEBUG_FOLDER, "proc_" + debug_name))
            except: pass

        with io.BytesIO() as output:
            image.save(output, format="PNG")
            processed_bytes = output.getvalue()
        
        # 3. CH·∫†Y OCR
        result, _ = ocr(processed_bytes)
        
        if result:
            # Thay v√¨ join th√¥ thi·ªÉn, ta g·ªçi h√†m s·∫Øp x·∫øp th√¥ng minh
            return adaptive_text_sorting(result)
            
    except Exception as e:
        print(f"      ‚ö†Ô∏è L·ªói x·ª≠ l√Ω ·∫£nh: {e}")
    return ""

    # --- H√ÄM M·ªöI: T√åM T√äN H√åNH (CAPTION) ---
def get_image_caption(page, img_rect):
    """
    Qu√©t v√πng vƒÉn b·∫£n ngay b√™n d∆∞·ªõi ·∫£nh ƒë·ªÉ t√¨m caption.
    V√≠ d·ª•: "H√¨nh 1: Ki·∫øn tr√∫c RAG", "Figure 2. Data Flow"
    """
    try:
        # 1. ƒê·ªãnh nghƒ©a v√πng qu√©t: Ngay b√™n d∆∞·ªõi ·∫£nh, cao kho·∫£ng 60px
        # (x0, y1, x1, y1 + 60) -> Qu√©t t·ª´ ch√¢n ·∫£nh xu·ªëng 60 ƒë∆°n v·ªã
        caption_rect = fitz.Rect(img_rect.x0, img_rect.y1, img_rect.x1, img_rect.y1 + 60)
        
        # 2. L·∫•y text trong v√πng ƒë√≥
        # clip=caption_rect: Ch·ªâ ƒë·ªçc ch·ªØ n·∫±m trong v√πng n√†y
        raw_caption = page.get_text("text", clip=caption_rect)
        
        # 3. L√†m s·∫°ch text
        clean_caption = clean_and_merge_lines(raw_caption)
        
        if not clean_caption:
            return None
            
        # 4. Ki·ªÉm tra xem c√≥ gi·ªëng caption kh√¥ng?
        # Th∆∞·ªùng caption s·∫Ω b·∫Øt ƒë·∫ßu b·∫±ng "H√¨nh", "Figure", "S∆° ƒë·ªì", "Fig"
        # Ho·∫∑c ƒë∆°n gi·∫£n l√† m·ªôt d√≤ng text ng·∫Øn (< 150 k√Ω t·ª±) n·∫±m ngay d∆∞·ªõi ·∫£nh
        keywords = ["H√¨nh", "Figure", "Fig", "S∆° ƒë·ªì", "Bi·ªÉu ƒë·ªì", "B·∫£ng"]
        
        # N·∫øu b·∫Øt ƒë·∫ßu b·∫±ng keyword HO·∫∂C ng·∫Øn v·ª´a ph·∫£i th√¨ l·∫•y
        if any(k in clean_caption for k in keywords) or len(clean_caption) < 150:
            return clean_caption
            
    except Exception:
        return None
    return None

def process_pdf(pdf_path):
    full_text_content = ""
    
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"‚ùå Kh√¥ng m·ªü ƒë∆∞·ª£c file: {e}")
        return []
    
    filename = os.path.basename(pdf_path)
    print(f"\nüìÑ ƒêang x·ª≠ l√Ω: {filename} ({len(doc)} trang)...")

    for i, page in enumerate(doc):
        # KHAI B√ÅO PAGE_NUM ·ªû ƒê√ÇY ƒê·ªÇ TR√ÅNH L·ªñI 'not defined'
        page_num = i + 1
        
        # 1. Text g·ªëc
        raw_text = page.get_text()
        clean_raw = clean_and_merge_lines(raw_text)
        
        # 2. X·ª≠ l√Ω ·∫¢nh (V√≤ng l·∫∑p Debug chi ti·∫øt)
        image_list = page.get_images(full=True)
        ocr_content = ""
        processed_any_image = False
        
        if image_list:
            print(f"   --- Trang {page_num}: C√≥ {len(image_list)} ·∫£nh.")
            
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    rects = page.get_image_rects(xref)
                    
                    for rect_idx, rect in enumerate(rects):
                        # In ra k√≠ch th∆∞·ªõc th·∫≠t
                        w, h = rect.width, rect.height
                        print(f"      üñºÔ∏è  ·∫¢nh {img_index}.{rect_idx} ({w:.0f}x{h:.0f}): ", end="")
                        
                        if w < 50 or h < 50: 
                            print("‚ùå B·ªé QUA (Qu√° nh·ªè)")
                            continue
                        
                        print("‚úÖ C·∫ÆT & OCR...", end=" ")
                        
                        try:
                            caption = get_image_caption(page, rect)
                        
                            # T·∫°o ti√™u ƒë·ªÅ header
                            if caption:
                                header_title = f"H√åNH ·∫¢NH: {caption}" # V√≠ d·ª•: H√åNH ·∫¢NH: H√¨nh 1. RAG
                                print(f"      üè∑Ô∏è  T√¨m th·∫•y caption: '{caption[:30]}...'")
                            else:
                                header_title = f"H√åNH ·∫¢NH (Trang {page_num})"
                            clip_rect = rect + (-10, -10, 10, 10)
                            pix = page.get_pixmap(clip=clip_rect, matrix=fitz.Matrix(3, 3))
                            
                            # Debug name ch·ª©a page_num
                            dbg_name = f"p{page_num}_img{img_index}_{rect_idx}.png"
                            
                            # G·ªçi h√†m OCR
                            text_in_image = process_image_for_ocr(pix.tobytes("png"), debug_name=dbg_name)
                            
                            # IN RA K·∫æT QU·∫¢ ƒê·ªÇ KI·ªÇM TRA
                            if not text_in_image:
                                print("‚ö†Ô∏è R·ªñNG (Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c ch·ªØ).")
                            elif len(text_in_image) <= 5:
                                print(f"‚ö†Ô∏è R√ÅC (Len={len(text_in_image)}): '{text_in_image}'")
                            else:
                                print(f"üéâ OK! ({len(text_in_image)} chars).")
                                ocr_content += (
                                    f"\n\n=== [{header_title}] ===\n"
                                    f"{text_in_image}\n"
                                    f"==============================\n"
                                )
                                processed_any_image = True
                                
                        except Exception as inner_e:
                            print(f"‚ùå L·ªói h√†m OCR: {inner_e}")

                except Exception as e:
                    print(f"\n      ‚ùå L·ªói v√≤ng l·∫∑p ·∫£nh: {e}")
                    continue

        # 3. Fallback Snapshot (N·∫øu kh√¥ng b·∫Øt ƒë∆∞·ª£c ·∫£nh n√†o m√† trang √≠t ch·ªØ)
        if not processed_any_image and len(clean_raw) < 300:
            print(f"   üì∏ Trang {page_num} √≠t text -> Th·ª≠ ch·ª•p to√†n trang...")
            try:
                pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))
                dbg_name = f"p{page_num}_snapshot.png"
                full_page_ocr = process_image_for_ocr(pix.tobytes("png"), debug_name=dbg_name)
                
                if len(full_page_ocr) > len(clean_raw) + 50:
                    print(f"      ‚úÖ Snapshot l·∫•y th√™m ƒë∆∞·ª£c {len(full_page_ocr)} k√Ω t·ª±.")
                    ocr_content += f"\n\n=== [SCAN TO√ÄN TRANG {page_num}] ===\n{full_page_ocr}\n==============================\n"
            except: pass

        # G·ªôp n·ªôi dung
        page_content = clean_raw + " " + ocr_content
        full_text_content += page_content + " "

    if full_text_content.strip():
        print(f"   ‚úÖ Xong file. T·ªïng: {len(full_text_content)} k√Ω t·ª±.")
        return [Document(
            page_content=full_text_content,
            metadata={
                "source": pdf_path,
                "filename": filename,
                "total_pages": len(doc)
            }
        )]
            
    return []


def load_all_documents():
    docs = []
    data_dir = "./data"
    
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        print("üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c data, h√£y copy file v√†o!")
        return []

    for root, _, files in os.walk(data_dir):
        for fname in files:
            path = os.path.join(root, fname)
            lname = fname.lower()
            try:
                if lname.endswith(".pdf"):
                    docs.extend(process_pdf(path))
                elif lname.endswith(".txt"):
                    print(f"üìÑ ƒêang x·ª≠ l√Ω TXT: {fname}")
                    loader = TextLoader(path, encoding="utf-8")
                    loaded = loader.load()
                    for d in loaded:
                        d.page_content = clean_and_merge_lines(d.page_content)
                        d.metadata["source"] = path
                        d.metadata["filename"] = fname
                    docs.extend(loaded)
            except Exception as e:
                print(f"‚ùå L·ªói x·ª≠ l√Ω file {fname}: {e}")
                
    print(f"\n‚úÖ T·ªîNG K·∫æT: {len(docs)} trang t√†i li·ªáu ƒë√£ s·∫µn s√†ng!")
    return docs

def run_ingest():
    print("üîÑ ƒêang qu√©t th∆∞ m·ª•c data...")
    raw_docs = load_all_documents()
    
    if not raw_docs:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o ho·∫∑c file r·ªóng!")
        return 

    print(f"\nüìö ƒêang chia nh·ªè (Chunking) {len(raw_docs)} trang t√†i li·ªáu...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)
    docs = text_splitter.split_documents(raw_docs)
    print(f"‚úÇÔ∏è ƒê√£ chia th√†nh {len(docs)} ƒëo·∫°n nh·ªè (chunks).")

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = PGVector(
        embeddings=embeddings,
        collection_name=COLLECTION_NAME,
        connection=DB_URL,
        use_jsonb=True,
    )
    namespace = f"pgvector/{COLLECTION_NAME}"

    def compute_checksum(text: str) -> str:
        return hashlib.sha256(text.encode("utf-8")).hexdigest()

    def ensure_table(conn):
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS ingest_records (
                    record_id TEXT PRIMARY KEY,
                    namespace TEXT,
                    source TEXT,
                    checksum TEXT,
                    updated_at TIMESTAMP
                )
            """)
            conn.commit()

    def load_existing(conn, namespace):
        with conn.cursor() as cur:
            cur.execute("SELECT record_id, checksum FROM ingest_records WHERE namespace = %s", (namespace,))
            return {row[0]: row[1] for row in cur.fetchall()}

    def upsert_record(conn, namespace, record_id, source, checksum):
        with conn.cursor() as cur:
            cur.execute("""
                INSERT INTO ingest_records(record_id, namespace, source, checksum, updated_at)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (record_id) DO UPDATE
                SET checksum = EXCLUDED.checksum, updated_at = EXCLUDED.updated_at
            """, (record_id, namespace, source, checksum, datetime.datetime.utcnow()))
            conn.commit()

    def delete_record(conn, namespace, record_id):
        with conn.cursor() as cur:
            cur.execute("DELETE FROM ingest_records WHERE namespace = %s AND record_id = %s", (namespace, record_id))
            conn.commit()

    try:
        clean_db_url = DB_URL.replace("postgresql+psycopg2://", "postgresql://")
        pg_conn = psycopg2.connect(clean_db_url)
    except Exception as e:
        print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi DATABASE_URL:", e)
        return

    ensure_table(pg_conn)

    print("\nüöÄ B·∫Øt ƒë·∫ßu ƒë·ªìng b·ªô d·ªØ li·ªáu...")

    existing = load_existing(pg_conn, namespace)
    current_record_ids = set()
    to_add = []
    to_update = []

    for i, doc in enumerate(docs):
        source_file = doc.metadata.get("filename", "unknown")
        record_id = f"{source_file}:{i}"
        checksum = compute_checksum(doc.page_content)
        current_record_ids.add(record_id)

        if record_id not in existing:
            to_add.append((record_id, doc, checksum, source_file))
        elif existing.get(record_id) != checksum:
            to_update.append((record_id, doc, checksum, source_file))

    def process_batch(items, is_update=False):
        if not items: return 0
        ids = [item[0] for item in items]
        batch_docs = [item[1] for item in items]
        action = "C·∫≠p nh·∫≠t" if is_update else "Th√™m m·ªõi"
        print(f"   Wait... ƒêang {action} {len(batch_docs)} vectors...")
        try:
            vector_store.add_documents(batch_docs, ids=ids)
            for rid, _, csum, src in items:
                upsert_record(pg_conn, namespace, rid, src, csum)
            return len(batch_docs)
        except Exception as e:
            print(f"   ‚ùå L·ªói Batch: {e}")
            return 0

    num_added = process_batch(to_add, is_update=False)
    num_updated = process_batch(to_update, is_update=True)

    existing_ids = set(existing.keys())
    to_delete = list(existing_ids - current_record_ids)
    num_deleted = 0

    if to_delete:
        print(f"   üóëÔ∏è ƒêang x√≥a {len(to_delete)} vectors c≈©...")
        try:
            vector_store.delete(ids=to_delete)
            for rid in to_delete:
                delete_record(pg_conn, namespace, rid)
                num_deleted += 1
        except Exception as e:
             print(f"‚ö†Ô∏è L·ªói x√≥a vector: {e}")

    pg_conn.close()

    print("\n‚úÖ HO√ÄN T·∫§T ƒê·ªíNG B·ªò!")
    print(f"   ‚ûï Th√™m: {num_added}")
    print(f"   üîÑ Update: {num_updated}")
    print(f"   üóëÔ∏è X√≥a: {num_deleted}")
    print(f"   ‚è≠Ô∏è B·ªè qua (Kh√¥ng ƒë·ªïi): {len(docs) - num_added - num_updated}")

if __name__ == "__main__":
    run_ingest()